{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:32:02.599891Z",
     "start_time": "2025-12-15T14:31:52.653770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    Dataset\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import tensorflow\n",
    "\n",
    "    print(\"⚠️ TensorFlow encore présent\")\n",
    "except ImportError:\n",
    "    print(\"✅ TensorFlow désinstallé\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    import torch\n",
    "\n",
    "    print(f\"✅ Transformers {transformers.__version__}\")\n",
    "    print(f\"✅ PyTorch {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ Transformers ou PyTorch manquant\")\n"
   ],
   "id": "786981d2ce6b1c50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow désinstallé\n",
      "✅ Transformers 4.57.3\n",
      "✅ PyTorch 2.9.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:32:07.041476Z",
     "start_time": "2025-12-15T14:32:07.001262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chargement dataset\n",
    "csv_path = \"../backend/app/data/raw/export_us_01.csv\"\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Fichier non trouvé: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, low_memory=False, sep=\";\", encoding=\"utf-8\")\n",
    "    print(f\"CSV file chargé avec succès. Nombre d'échantillons chargés: {df.shape[0]}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ],
   "id": "7dd82db182073cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file chargé avec succès. Nombre d'échantillons chargés: 1118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  Issue Type         Key Priority     Status  \\\n",
       "0      Story  CODEE01-35      NaN  Cancelled   \n",
       "1      Story  CODEE01-91      NaN  Cancelled   \n",
       "2      Story  CODEE01-90      NaN  Cancelled   \n",
       "3      Story  CODEE01-55      NaN  Cancelled   \n",
       "4      Story  CODEE01-65      NaN  Cancelled   \n",
       "\n",
       "                                             Summary     Created  \\\n",
       "0  [eShop] [OUT OF SCOPE] Products recommendation...  27-03-2023   \n",
       "1        [Go-live] Activation of a B2B portal itself  27-03-2023   \n",
       "2  [Go-live] Activation of the integration with o...  27-03-2023   \n",
       "3  [Integration with CMS][OUT of the Scope]Integr...  27-03-2023   \n",
       "4  [General] Configuration of the Version Control...  27-03-2023   \n",
       "\n",
       "                                         Description Unnamed: 7 Unnamed: 8  \\\n",
       "0  As a business \\n\\nI want to system supports pr...        NaN        NaN   \n",
       "1  As a Tech \\n\\nI want to story \\n\\nSo that I ca...        NaN        NaN   \n",
       "2  As a Tech \\n\\nI want to story \\n\\nSo that I ca...        NaN        NaN   \n",
       "3  As a business \\n\\nI want to show to clients th...        NaN        NaN   \n",
       "4  As a Tech \\n\\nI want to story \\n\\nSo that I ca...        NaN        NaN   \n",
       "\n",
       "  Unnamed: 9 Unnamed: 10 Unnamed: 11  \n",
       "0        NaN         NaN         NaN  \n",
       "1        NaN         NaN         NaN  \n",
       "2        NaN         NaN         NaN  \n",
       "3        NaN         NaN         NaN  \n",
       "4        NaN         NaN         NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue Type</th>\n",
       "      <th>Key</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Status</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Created</th>\n",
       "      <th>Description</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story</td>\n",
       "      <td>CODEE01-35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>[eShop] [OUT OF SCOPE] Products recommendation...</td>\n",
       "      <td>27-03-2023</td>\n",
       "      <td>As a business \\n\\nI want to system supports pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Story</td>\n",
       "      <td>CODEE01-91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>[Go-live] Activation of a B2B portal itself</td>\n",
       "      <td>27-03-2023</td>\n",
       "      <td>As a Tech \\n\\nI want to story \\n\\nSo that I ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Story</td>\n",
       "      <td>CODEE01-90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>[Go-live] Activation of the integration with o...</td>\n",
       "      <td>27-03-2023</td>\n",
       "      <td>As a Tech \\n\\nI want to story \\n\\nSo that I ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Story</td>\n",
       "      <td>CODEE01-55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>[Integration with CMS][OUT of the Scope]Integr...</td>\n",
       "      <td>27-03-2023</td>\n",
       "      <td>As a business \\n\\nI want to show to clients th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Story</td>\n",
       "      <td>CODEE01-65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>[General] Configuration of the Version Control...</td>\n",
       "      <td>27-03-2023</td>\n",
       "      <td>As a Tech \\n\\nI want to story \\n\\nSo that I ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Préprocessing\n",
   "id": "12832852a344fd1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:32:10.177306Z",
     "start_time": "2025-12-15T14:32:10.168471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clean data\n",
    "columns = ['Key', 'Created']\n",
    "df.drop(columns=columns, inplace=True)\n",
    "\n",
    "features = ['Issue Type', 'Summary', 'Description']\n",
    "contentX = df[features].copy()\n",
    "contentX.fillna(\"\")"
   ],
   "id": "55e92ab267179a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Issue Type                                            Summary  \\\n",
       "0         Story  [eShop] [OUT OF SCOPE] Products recommendation...   \n",
       "1         Story        [Go-live] Activation of a B2B portal itself   \n",
       "2         Story  [Go-live] Activation of the integration with o...   \n",
       "3         Story  [Integration with CMS][OUT of the Scope]Integr...   \n",
       "4         Story  [General] Configuration of the Version Control...   \n",
       "...         ...                                                ...   \n",
       "1113                                                                 \n",
       "1114                                                                 \n",
       "1115                                                                 \n",
       "1116                                                                 \n",
       "1117                                                                 \n",
       "\n",
       "                                            Description  \n",
       "0     As a business \\n\\nI want to system supports pr...  \n",
       "1     As a Tech \\n\\nI want to story \\n\\nSo that I ca...  \n",
       "2     As a Tech \\n\\nI want to story \\n\\nSo that I ca...  \n",
       "3     As a business \\n\\nI want to show to clients th...  \n",
       "4     As a Tech \\n\\nI want to story \\n\\nSo that I ca...  \n",
       "...                                                 ...  \n",
       "1113                                                     \n",
       "1114                                                     \n",
       "1115                                                     \n",
       "1116                                                     \n",
       "1117                                                     \n",
       "\n",
       "[1118 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue Type</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story</td>\n",
       "      <td>[eShop] [OUT OF SCOPE] Products recommendation...</td>\n",
       "      <td>As a business \\n\\nI want to system supports pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Story</td>\n",
       "      <td>[Go-live] Activation of a B2B portal itself</td>\n",
       "      <td>As a Tech \\n\\nI want to story \\n\\nSo that I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Story</td>\n",
       "      <td>[Go-live] Activation of the integration with o...</td>\n",
       "      <td>As a Tech \\n\\nI want to story \\n\\nSo that I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Story</td>\n",
       "      <td>[Integration with CMS][OUT of the Scope]Integr...</td>\n",
       "      <td>As a business \\n\\nI want to show to clients th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Story</td>\n",
       "      <td>[General] Configuration of the Version Control...</td>\n",
       "      <td>As a Tech \\n\\nI want to story \\n\\nSo that I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1118 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:32:14.538593Z",
     "start_time": "2025-12-15T14:32:14.480486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def nettoyer_texte_description(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', ' ', text)  # espaces multiples\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n', text)  # supprime lignes vides multiples\n",
    "    text = text.replace(r'\\n\\s*\\n+', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "# Nettoie et normalise le texte\n",
    "def nettoyer_texte(texte):\n",
    "    texte = re.sub(r\"([.,!?'])\", r\" \\1 \", texte)\n",
    "    texte = re.sub(r\"([-●'])\", r\" \", texte)\n",
    "    return texte.strip()\n",
    "\n",
    "\n",
    "def extract_acceptance_criteria(text: str) -> List[str]:\n",
    "    text = text.replace(\"●\", \"-\")  # Remplace les bullets non standard par \"-\"\n",
    "\n",
    "    # Trouver la section \"Acceptance Criteria\"\n",
    "    match = re.search(r'Acceptance Criteria(.*)', text, re.DOTALL | re.IGNORECASE)\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    ac_section = match.group(1).strip()\n",
    "\n",
    "    # Découper selon les puces commençant par \"-\"\n",
    "    items = re.split(r'-\\s*', ac_section)\n",
    "    items = [i.strip() for i in items if i.strip()]\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def safe_text(v):\n",
    "    if isinstance(v, float):  # couvre NaN ou nombres\n",
    "        return \"\"\n",
    "    return str(v).strip()\n",
    "\n",
    "\n",
    "def preprocess_issueType(raw_text: str) -> str:\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def preprocess_summary(raw_text: str) -> str:\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def preprocess_description(raw_text: str):\n",
    "    text = nettoyer_texte_description(raw_text)\n",
    "\n",
    "    # Résumé auto : première phrase \"As a business...\"\n",
    "    summary_match = re.search(r\"As a .*?[\\.\\n]\", text, re.IGNORECASE)\n",
    "    summary = summary_match.group(0).strip() if summary_match else \"\"\n",
    "\n",
    "    # Description : la partie avant les critères d'acceptation\n",
    "    description = re.split(r'Acceptance Criteria', text, flags=re.IGNORECASE)[0]\n",
    "    description = nettoyer_texte_description(description)\n",
    "\n",
    "    # Critères d'acceptation\n",
    "    acceptance_criteria = extract_acceptance_criteria(text)\n",
    "    acceptance_criterias = '\\n - '.join(acceptance_criteria)\n",
    "    return {\n",
    "        'content_summary': summary,\n",
    "        'description': description,\n",
    "        'acceptance_criteria': acceptance_criterias,\n",
    "    }\n",
    "\n",
    "\n",
    "resp = []\n",
    "\n",
    "for i, phrase in contentX.iterrows():\n",
    "    rawIssueType = safe_text(phrase['Issue Type'])\n",
    "    if rawIssueType == \"\":\n",
    "        rawIssueType = \"Story\"  # set default value\n",
    "    rawSummary = safe_text(phrase['Summary'])\n",
    "    if rawSummary == \"\":\n",
    "        rawIssueType = \"Empty\"  # set default value\n",
    "    rawDescription = safe_text(phrase['Description'])\n",
    "    # Skip empty info\n",
    "    if rawDescription != \"\":\n",
    "        issueType = preprocess_issueType(rawIssueType)\n",
    "        summary = preprocess_summary(rawSummary)\n",
    "        description = preprocess_description(rawDescription)\n",
    "        resp.append(\n",
    "            {\n",
    "                \"issue_type\": nettoyer_texte(issueType),\n",
    "                \"summary\": nettoyer_texte(rawSummary),\n",
    "                \"content_summary\": nettoyer_texte(description['content_summary']),\n",
    "                \"description\": nettoyer_texte(description['description']),\n",
    "                \"acceptance_criteria\": nettoyer_texte(description['acceptance_criteria']),\n",
    "            })\n",
    "\n",
    "print(f\"Size : {len(resp)}\")\n"
   ],
   "id": "b2098b07c994fa95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : 877\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:32:20.094442Z",
     "start_time": "2025-12-15T14:32:20.091998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for i, texte in enumerate(resp[:5]):\n",
    "    print(f\"{i}.\\n{texte}\\n\")\n"
   ],
   "id": "ff1f0a18eeb39e9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.\n",
      "{'issue_type': 'Story', 'summary': '[eShop] [OUT OF SCOPE] Products recommendation (AI)', 'content_summary': 'As a business', 'description': 'As a business \\nI want to system supports products recommendation based on AI \\nSo that I can provide my clients better experience and the most relevant suggestions', 'acceptance_criteria': 'Universal custom LWC component with the list of recommended products that can be placed to any Community page . \\n   The Custom component will use the Einstein API for getting insights from the AI . \\n   The Einstein need to be enabled in the B2B portal once system will be launched and enough data(orders ,  cartItems) will be created there . \\n   Einstein will be turned on without custom LWC component'}\n",
      "\n",
      "1.\n",
      "{'issue_type': 'Story', 'summary': '[Go live] Activation of a B2B portal itself', 'content_summary': 'As a Tech', 'description': 'As a Tech \\nI want to story \\nSo that I can N/A', 'acceptance_criteria': ''}\n",
      "\n",
      "2.\n",
      "{'issue_type': 'Story', 'summary': '[Go live] Activation of the integration with other systems: QAD ,  MDM', 'content_summary': 'As a Tech', 'description': 'As a Tech \\nI want to story \\nSo that I can N/A', 'acceptance_criteria': ''}\n",
      "\n",
      "3.\n",
      "{'issue_type': 'Story', 'summary': '[Integration with CMS][OUT of the Scope]Integration setup between SF and CMS', 'content_summary': 'As a business', 'description': 'As a business \\nI want to show to clients the same products related content at eShop and Web site \\nSo that I can have consistency and up to date products information on all platforms', 'acceptance_criteria': 'Setup credentials\\n   Setup connection settings'}\n",
      "\n",
      "4.\n",
      "{'issue_type': 'Story', 'summary': '[General] Configuration of the Version Control System', 'content_summary': 'As a Tech', 'description': 'As a Tech \\nI want to story \\nSo that I can N/A', 'acceptance_criteria': 'Setup Git repository , access to it'}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:32:22.068147Z",
     "start_time": "2025-12-15T14:32:22.029650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define device\n",
    "\n",
    "use_mps = False\n",
    "use_fp16 = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_fp16 = True\n",
    "    print(f\"GPU NVIDIA détecté: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_mps = True\n",
    "    use_fp16 = False\n",
    "    print(\"GPU Apple Silicon (MPS) détecté\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_mps = False\n",
    "    use_fp16 = False\n",
    "    print(\"CPU détecté\")\n",
    "\n",
    "print(f\"Model defined {device}\")"
   ],
   "id": "1a1aaf525dfd32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Apple Silicon (MPS) détecté\n",
      "Model defined mps\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:48:35.200636Z",
     "start_time": "2025-12-15T14:48:29.480404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "autoTokenizerGen = AutoTokenizer.from_pretrained(model_name)\n",
    "modelGen = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "modelGen.to(device)\n",
    "\n",
    "\n",
    "def normalize_tags(tag_str):\n",
    "    # split by comma, strip, lowercase, replace spaces by hyphen, remove duplicates\n",
    "    tags = [t.strip().lower().replace(\" \", \"-\") for t in tag_str.split(\",\") if t.strip()]\n",
    "    seen = []\n",
    "    for t in tags:\n",
    "        if t not in seen:\n",
    "            seen.append(t)\n",
    "    return seen\n",
    "\n",
    "\n",
    "def generate_client_sentence(target, max_length=64):\n",
    "    def format_prompt(target, num_tags=8):\n",
    "        return f\"\"\"\n",
    "        Generate {num_tags} relevant tags for this description.\n",
    "        Tags should be lowercase, comma-separated, and include technologies, frameworks, and project type.\n",
    "\n",
    "        Project description: {target}\n",
    "\n",
    "        Tags:\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = format_prompt(target)\n",
    "\n",
    "    inputs_gen = autoTokenizerGen(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_gen = modelGen.generate(\n",
    "            **inputs_gen,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    tags = autoTokenizerGen.decode(outputs_gen[0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean up output\n",
    "    tags = tags.strip()\n",
    "    if not tags:\n",
    "        return \"web-app, software, development\"  # Fallback\n",
    "\n",
    "    return {\n",
    "        \"target\": target,\n",
    "        \"targs\": tags,\n",
    "    }\n"
   ],
   "id": "63f7ac5723a5752b",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:48:57.387291Z",
     "start_time": "2025-12-15T14:48:54.780218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_input_description(tags):\n",
    "    return f\"\"\"\n",
    "    Analyze the following tags relevant for this description\n",
    "    Extract only the relevant business requirement and create a complete Agile User Story.\n",
    "    Tags: {tags}\n",
    "    User Story:\n",
    "    \"\"\"\n",
    "\n",
    "content_data = []\n",
    "for t in resp:\n",
    "    data = generate_client_sentence(t['description'])\n",
    "    data_input = generate_input_description(data['tags'])\n",
    "    content_data.append({\n",
    "        \"input\": data_input,\n",
    "        \"output\": data['target'],\n",
    "    })\n"
   ],
   "id": "3b3e05dc447f5097",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:50:01.521636Z",
     "start_time": "2025-12-15T14:50:01.515447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, cd in enumerate(content_data[:5]):\n",
    "    print(f\"{i}.\\Input: \\n{cd[\"input\"]}\\nOutput: \\n{cd['output']}\\n\")"
   ],
   "id": "7dcef70266c86d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.\\Input: \n",
      "generate description: technologies, frameworks, project type\n",
      "Output: \n",
      "As a business \n",
      "I want to system supports products recommendation based on AI \n",
      "So that I can provide my clients better experience and the most relevant suggestions\n",
      "\n",
      "1.\\Input: \n",
      "generate description: project type\n",
      "Output: \n",
      "As a Tech \n",
      "I want to story \n",
      "So that I can N/A\n",
      "\n",
      "2.\\Input: \n",
      "generate description: project type\n",
      "Output: \n",
      "As a Tech \n",
      "I want to story \n",
      "So that I can N/A\n",
      "\n",
      "3.\\Input: \n",
      "generate description: technologies, frameworks, project type\n",
      "Output: \n",
      "As a business \n",
      "I want to show to clients the same products related content at eShop and Web site \n",
      "So that I can have consistency and up to date products information on all platforms\n",
      "\n",
      "4.\\Input: \n",
      "generate description: project type\n",
      "Output: \n",
      "As a Tech \n",
      "I want to story \n",
      "So that I can N/A\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:50:11.031637Z",
     "start_time": "2025-12-15T14:50:10.998062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split = Dataset.from_list(content_data).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": split[\"test\"]\n",
    "})\n",
    "\n",
    "print(len(dataset_dict[\"train\"]))\n",
    "print(len(dataset_dict[\"validation\"]))"
   ],
   "id": "7e49422a138fcbca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:50:15.409395Z",
     "start_time": "2025-12-15T14:50:14.866317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the Tokenizer\n",
    "#model_name = \"t5-small\"\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = batch[\"input\"]\n",
    "    targets = batch[\"output\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized = dataset_dict.map(preprocess, batched=True, remove_columns=dataset_dict[\"train\"].column_names)\n",
    "tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ],
   "id": "c9cc3ad0122340a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 18/18 [00:00<00:00, 1368.85 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 971.02 examples/s]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:50:36.671800Z",
     "start_time": "2025-12-15T14:50:18.849141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # defines the rank of the update matrices\n",
    "    lora_alpha=32,  # scales the updates\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\", \"all-linear\"],  # Adjust based on model architecture - attention projection modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  # sequence-to-sequence task\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Paramétres\n",
    "EPOCHS = 25  #25\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "train_batch_size = 8 if use_mps else 4\n",
    "eval_batch_size = 8 if use_mps else 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../target/t5_tag_generator\",\n",
    "\n",
    "    # Paramètres d'entraînement\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "\n",
    "    # Paramètres essentiels selon le device\n",
    "    fp16=use_fp16,  # True si GPU\n",
    "    use_mps_device=use_mps,  # True si GPU mps détecté\n",
    "\n",
    "    # Optimisation\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ],
   "id": "35b091cefe05d3c8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:53:51.662375Z",
     "start_time": "2025-12-15T14:53:33.416965Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "7c688112cbb73345",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/github-projets/SoftwareCrewIA/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2323\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2324\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2325\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2326\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2327\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2328\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/github-projets/SoftwareCrewIA/.venv/lib/python3.13/site-packages/transformers/trainer.py:2356\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2353\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_inner_training_loop\u001B[39m(\n\u001B[32m   2354\u001B[39m     \u001B[38;5;28mself\u001B[39m, batch_size=\u001B[38;5;28;01mNone\u001B[39;00m, args=\u001B[38;5;28;01mNone\u001B[39;00m, resume_from_checkpoint=\u001B[38;5;28;01mNone\u001B[39;00m, trial=\u001B[38;5;28;01mNone\u001B[39;00m, ignore_keys_for_eval=\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2355\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m2356\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfree_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2357\u001B[39m     \u001B[38;5;28mself\u001B[39m._train_batch_size = batch_size\n\u001B[32m   2358\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.auto_find_batch_size:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/github-projets/SoftwareCrewIA/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:3918\u001B[39m, in \u001B[36mAccelerator.free_memory\u001B[39m\u001B[34m(self, *objects)\u001B[39m\n\u001B[32m   3916\u001B[39m         \u001B[38;5;28mself\u001B[39m.deepspeed_engine_wrapped.engine.destroy()\n\u001B[32m   3917\u001B[39m     \u001B[38;5;28mself\u001B[39m.deepspeed_engine_wrapped = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3918\u001B[39m objects = \u001B[43mrelease_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mobjects\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3919\u001B[39m \u001B[38;5;28mself\u001B[39m._schedulers = []\n\u001B[32m   3920\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizers = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/github-projets/SoftwareCrewIA/.venv/lib/python3.13/site-packages/accelerate/utils/memory.py:97\u001B[39m, in \u001B[36mrelease_memory\u001B[39m\u001B[34m(*objects)\u001B[39m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(objects)):\n\u001B[32m     96\u001B[39m     objects[i] = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[43mclear_device_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgarbage_collection\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m objects\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/github-projets/SoftwareCrewIA/.venv/lib/python3.13/site-packages/accelerate/utils/memory.py:63\u001B[39m, in \u001B[36mclear_device_cache\u001B[39m\u001B[34m(garbage_collection)\u001B[39m\n\u001B[32m     61\u001B[39m     torch.npu.empty_cache()\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_mps_available(min_version=\u001B[33m\"\u001B[39m\u001B[33m2.0\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmps\u001B[49m\u001B[43m.\u001B[49m\u001B[43mempty_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_cuda_available():\n\u001B[32m     65\u001B[39m     torch.cuda.empty_cache()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/github-projets/SoftwareCrewIA/.venv/lib/python3.13/site-packages/torch/mps/__init__.py:86\u001B[39m, in \u001B[36mempty_cache\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mempty_cache\u001B[39m() -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     83\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[33;03m    allocator so that those can be used in other GPU applications.\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m     \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_mps_emptyCache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:38:12.273270Z",
     "start_time": "2025-12-15T14:38:12.268326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_description(query, model, tokenizer, max_length=512, num_beams=5):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(\n",
    "        model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id  #  required for T5\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "id": "f015496110c0582b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:37:06.451263Z",
     "start_time": "2025-12-15T14:37:05.458517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enregistrement du modèle\n",
    "x = datetime.datetime.now()\n",
    "x = x.strftime(\"%Y-%m-%d.%H:%M:%S\")\n",
    "#modelName = f\"model_0_{x}.pkl\"\n",
    "modelName = f\"model_0.pkl\"\n",
    "\n",
    "print(f\"• enregistrement du modèle {modelName}\")\n",
    "joblib.dump(model, \"../backend/models/\" + modelName)\n",
    "print(\"• Fin de l'enregistrement' du modèle\")"
   ],
   "id": "4626f1baa3523ddd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• enregistrement du modèle model_1.pkl\n",
      "• Fin de l'enregistrement' du modèle\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T14:39:55.229494Z",
     "start_time": "2025-12-15T14:39:51.053889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_input(tags):\n",
    "    return f\"Write a short project summary using: {tags}\"\n",
    "\n",
    "\n",
    "modelPath = \"../backend/models/\" + modelName\n",
    "loadedModel = joblib.load(modelPath)\n",
    "loadedModel.to(device)\n",
    "lastPrediction = generate_description(\n",
    "    format_input(\"technologies, frameworks, project type\"),\n",
    "    loadedModel,\n",
    "    tokenizer)\n",
    "\n",
    "print(\"Prédiction : \" + lastPrediction)\n"
   ],
   "id": "5cfca1e152f32f30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction : technology frameworks and project type\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
