{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    Dataset\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import tensorflow\n",
    "\n",
    "    print(\"⚠️ TensorFlow encore présent\")\n",
    "except ImportError:\n",
    "    print(\"✅ TensorFlow désinstallé\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    import torch\n",
    "\n",
    "    print(f\"✅ Transformers {transformers.__version__}\")\n",
    "    print(f\"✅ PyTorch {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ Transformers ou PyTorch manquant\")\n"
   ],
   "id": "786981d2ce6b1c50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Chargement dataset\n",
    "csv_path = \"../backend/app/data/raw/export_us_01.csv\"\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Fichier non trouvé: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, low_memory=False, sep=\";\", encoding=\"utf-8\")\n",
    "    print(f\"CSV file chargé avec succès. Nombre d'échantillons chargés: {df.shape[0]}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ],
   "id": "7dd82db182073cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Préprocessing\n",
   "id": "12832852a344fd1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean data\n",
    "columns = ['Key', 'Created']\n",
    "df.drop(columns=columns, inplace=True)\n",
    "\n",
    "features = ['Issue Type', 'Summary', 'Description']\n",
    "contentX = df[features].copy()\n",
    "contentX.fillna(\"\")"
   ],
   "id": "55e92ab267179a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def nettoyer_texte_description(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', ' ', text)  # espaces multiples\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n', text)  # supprime lignes vides multiples\n",
    "    text = text.replace(r'\\n\\s*\\n+', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "# Nettoie et normalise le texte\n",
    "def nettoyer_texte(texte):\n",
    "    texte = re.sub(r\"([.,!?'])\", r\" \\1 \", texte)\n",
    "    texte = re.sub(r\"([-●'])\", r\" \", texte)\n",
    "    return texte.strip()\n",
    "\n",
    "\n",
    "def extract_acceptance_criteria(text: str) -> List[str]:\n",
    "    text = text.replace(\"●\", \"-\")  # Remplace les bullets non standard par \"-\"\n",
    "\n",
    "    # Trouver la section \"Acceptance Criteria\"\n",
    "    match = re.search(r'Acceptance Criteria(.*)', text, re.DOTALL | re.IGNORECASE)\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    ac_section = match.group(1).strip()\n",
    "\n",
    "    # Découper selon les puces commençant par \"-\"\n",
    "    items = re.split(r'-\\s*', ac_section)\n",
    "    items = [i.strip() for i in items if i.strip()]\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def safe_text(v):\n",
    "    if isinstance(v, float):  # couvre NaN ou nombres\n",
    "        return \"\"\n",
    "    return str(v).strip()\n",
    "\n",
    "\n",
    "def preprocess_issueType(raw_text: str) -> str:\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def preprocess_summary(raw_text: str) -> str:\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def preprocess_description(raw_text: str):\n",
    "    text = nettoyer_texte_description(raw_text)\n",
    "\n",
    "    # Résumé auto : première phrase \"As a business...\"\n",
    "    summary_match = re.search(r\"As a .*?[\\.\\n]\", text, re.IGNORECASE)\n",
    "    summary = summary_match.group(0).strip() if summary_match else \"\"\n",
    "\n",
    "    # Description : la partie avant les critères d'acceptation\n",
    "    description = re.split(r'Acceptance Criteria', text, flags=re.IGNORECASE)[0]\n",
    "    description = nettoyer_texte_description(description)\n",
    "\n",
    "    # Critères d'acceptation\n",
    "    acceptance_criteria = extract_acceptance_criteria(text)\n",
    "    acceptance_criterias = '\\n - '.join(acceptance_criteria)\n",
    "    return {\n",
    "        'content_summary': summary,\n",
    "        'description': description,\n",
    "        'acceptance_criteria': acceptance_criterias,\n",
    "    }\n",
    "\n",
    "\n",
    "resp = []\n",
    "\n",
    "for i, phrase in contentX.iterrows():\n",
    "    rawIssueType = safe_text(phrase['Issue Type'])\n",
    "    if rawIssueType == \"\":\n",
    "        rawIssueType = \"Story\" # set default value\n",
    "    rawSummary = safe_text(phrase['Summary'])\n",
    "    if rawSummary == \"\":\n",
    "        rawIssueType = \"Empty\" # set default value\n",
    "    rawDescription = safe_text(phrase['Description'])\n",
    "    # Skip empty info\n",
    "    if rawDescription != \"\":\n",
    "        issueType = preprocess_issueType(rawIssueType)\n",
    "        summary = preprocess_summary(rawSummary)\n",
    "        description = preprocess_description(rawDescription)\n",
    "        resp.append(\n",
    "            {\n",
    "                \"issue_type\": nettoyer_texte(issueType),\n",
    "                \"summary\": nettoyer_texte(rawSummary),\n",
    "                \"content_summary\": nettoyer_texte(description['content_summary']),\n",
    "                \"description\": nettoyer_texte(description['description']),\n",
    "                \"acceptance_criteria\": nettoyer_texte(description['acceptance_criteria']),\n",
    "            })\n",
    "\n",
    "print(f\"Size : {len(resp)}\")\n"
   ],
   "id": "b2098b07c994fa95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "for i, texte in enumerate(resp[:5]):\n",
    "    print(f\"{i}.\\n{texte}\\n\")\n"
   ],
   "id": "ff1f0a18eeb39e9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define device\n",
    "\n",
    "use_mps = False\n",
    "use_fp16 = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_fp16 = True\n",
    "    print(f\"GPU NVIDIA détecté: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_mps = True\n",
    "    use_fp16 = False\n",
    "    print(\"GPU Apple Silicon (MPS) détecté\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_mps = False\n",
    "    use_fp16 = False\n",
    "    print(\"CPU détecté\")\n",
    "\n",
    "print(f\"Model defined {device}\")"
   ],
   "id": "1a1aaf525dfd32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "autoTokenizerGen = AutoTokenizer.from_pretrained(model_name)\n",
    "modelGen = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "modelGen.to(device)\n",
    "\n",
    "def normalize_tags(tag_str):\n",
    "    # split by comma, strip, lowercase, replace spaces by hyphen, remove duplicates\n",
    "    tags = [t.strip().lower().replace(\" \", \"-\") for t in tag_str.split(\",\") if t.strip()]\n",
    "    seen = []\n",
    "    for t in tags:\n",
    "        if t not in seen:\n",
    "            seen.append(t)\n",
    "    return seen\n",
    "\n",
    "\n",
    "def generate_client_sentence(target, max_length=128):\n",
    "    num_tags = 8\n",
    "    prompt = (\n",
    "        f\"\"\"Generate {num_tags} relevant tags for this description.\n",
    "        Tags should be lowercase, comma-separated, and include technologies, frameworks, and project type.\n",
    "\n",
    "        Project description: {target}\n",
    "\n",
    "        Tags:\"\"\"\n",
    "    )\n",
    "\n",
    "    inputsGen = autoTokenizerGen(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputsGen = modelGen.generate(\n",
    "            **inputsGen,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    tags = autoTokenizerGen.decode(outputsGen[0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean up output\n",
    "    tags = tags.strip()\n",
    "    if not tags:\n",
    "        return \"web-app, software, development\"  # Fallback\n",
    "\n",
    "    return {\n",
    "        \"target\": target,\n",
    "        \"targs\": tags,\n",
    "    }\n"
   ],
   "id": "63f7ac5723a5752b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "content_data = []\n",
    "for t in resp:\n",
    "    data = generate_client_sentence(t['description'])\n",
    "    content_data.append({\n",
    "        \"input\": data['targs'],\n",
    "        \"output\": data['target'],\n",
    "    })\n"
   ],
   "id": "3b3e05dc447f5097"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i, cd in enumerate(content_data[:5]):\n",
    "    print(f\"{i}.\\Input: \\n{cd[\"input\"]}\\nOutput: \\n{cd['output']}\\n\")"
   ],
   "id": "7dcef70266c86d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "split = Dataset.from_list(content_data).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": split[\"test\"]\n",
    "})\n",
    "\n",
    "print(len(dataset_dict[\"train\"]))\n",
    "print(len(dataset_dict[\"validation\"]))"
   ],
   "id": "7e49422a138fcbca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the Tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = batch[\"input\"]\n",
    "    targets = batch[\"output\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset_dict.map(preprocess, batched=True, remove_columns=dataset_dict[\"train\"].column_names)\n",
    "tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ],
   "id": "c9cc3ad0122340a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # defines the rank of the update matrices\n",
    "    lora_alpha=32,  # scales the updates\n",
    "    target_modules=[\"q\", \"v\"],  # Adjust based on model architecture - attention projection modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  # sequence-to-sequence task\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Paramétres\n",
    "EPOCHS = 25  #25\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "train_batch_size = 8 if use_mps else 4\n",
    "eval_batch_size = 8 if use_mps else 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../target/t5_tag_generator\",\n",
    "\n",
    "    # Paramètres d'entraînement\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "\n",
    "    # Paramètres essentiels selon le device\n",
    "    fp16=use_fp16,  # True si GPU\n",
    "    use_mps_device=use_mps,  # True si GPU mps détecté\n",
    "\n",
    "    # Optimisation\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ],
   "id": "35b091cefe05d3c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "7c688112cbb73345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_tags(query, model, tokenizer, max_length=512, num_beams=5):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id  #  required for T5\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "id": "f015496110c0582b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Enregistrement du modèle\n",
    "x = datetime.datetime.now()\n",
    "x = x.strftime(\"%Y-%m-%d.%H:%M:%S\")\n",
    "#modelName = f\"model_0_{x}.pkl\"\n",
    "modelName = f\"model_0.pkl\"\n",
    "\n",
    "print(f\"• enregistrement du modèle {modelName}\")\n",
    "joblib.dump(model, \"../backend/models/\" + modelName)\n",
    "print(\"• Fin de l'enregistrement' du modèle\")"
   ],
   "id": "4626f1baa3523ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T22:22:33.345153Z",
     "start_time": "2025-12-14T22:22:28.550824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "modelPath = \"../backend/models/\" + modelName\n",
    "loadedModel = joblib.load(modelPath)\n",
    "loadedModel.to(device)\n",
    "lastPrediction = generate_tags(\n",
    "    \"technologies, frameworks, project type\",\n",
    "    loadedModel,\n",
    "    tokenizer)\n",
    "\n",
    "print(\"Prédiction : \" + lastPrediction)\n"
   ],
   "id": "5cfca1e152f32f30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction : Technologies, frameworks, project type technologies, frameworks, project type technologies, frameworks, frameworks, project type technologies, frameworks, frameworks, project type technologies, frameworks, frameworks, project type technologies, frameworks, frameworks, project type technologies, frameworks, frameworks, project type technologies, frameworks, frameworks, project type technologies, frameworks, frameworks, frameworks, project type technologies, frameworks, frameworks, frameworks, frameworks, project type technologies, frameworks, frameworks, project type\n"
     ]
    }
   ],
   "execution_count": 158
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
